<!doctype html><html lang=en><head><title>Improving Model Performance with Cleanlab · ChristopherMauck</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta http-equiv=content-security-policy content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net/; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;"><meta name=author content="Christopher Mauck"><meta name=description content="Improving Model Performance with Cleanlab Link to heading Can we improve performance on baseline models? Link to heading To offer some inspiration for this, the data that I used comes from a project I did my junior year at MIT. A group of friends and I took it upon ourselves to try and &ldquo;beat&rdquo; Vegas at predicting NFL game outcomes. Using their &ldquo;line&rdquo; to determine which team they believed would win, we used our classification model (a GBC) to make our predictions."><meta name=keywords content="blog,developer,personal,cleanlab"><meta name=twitter:card content="summary"><meta name=twitter:title content="Improving Model Performance with Cleanlab"><meta name=twitter:description content="Improving Model Performance with Cleanlab Link to heading Can we improve performance on baseline models? Link to heading To offer some inspiration for this, the data that I used comes from a project I did my junior year at MIT. A group of friends and I took it upon ourselves to try and &ldquo;beat&rdquo; Vegas at predicting NFL game outcomes. Using their &ldquo;line&rdquo; to determine which team they believed would win, we used our classification model (a GBC) to make our predictions."><meta property="og:title" content="Improving Model Performance with Cleanlab"><meta property="og:description" content="Improving Model Performance with Cleanlab Link to heading Can we improve performance on baseline models? Link to heading To offer some inspiration for this, the data that I used comes from a project I did my junior year at MIT. A group of friends and I took it upon ourselves to try and &ldquo;beat&rdquo; Vegas at predicting NFL game outcomes. Using their &ldquo;line&rdquo; to determine which team they believed would win, we used our classification model (a GBC) to make our predictions."><meta property="og:type" content="article"><meta property="og:url" content="https://cmauck10.github.io/posts/my-second-post/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-08-26T11:08:04-05:00"><meta property="article:modified_time" content="2022-08-26T11:08:04-05:00"><link rel=canonical href=https://cmauck10.github.io/posts/my-second-post/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.c4d7e93a158eda5a65b3df343745d2092a0a1e2170feeec909b8a89443903c6a.css integrity="sha256-xNfpOhWO2lpls980N0XSCSoKHiFw/u7JCbiolEOQPGo=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.39e41a7f16bdf8cb16e43cae7d714fa1016f1d2d2898a5b3f27f42c9979204e2.css integrity="sha256-OeQafxa9+MsW5DyufXFPoQFvHS0omKWz8n9CyZeSBOI=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5><meta name=generator content="Hugo 0.101.0"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>ChristopherMauck</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/about/>About</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://cmauck10.github.io/posts/my-second-post/>Improving Model Performance with Cleanlab</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2022-08-26T11:08:04-05:00>August 26, 2022</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
5-minute read</span></div></div></header><div><h1 id=improving-model-performance-with-cleanlab>Improving Model Performance with Cleanlab
<a class=heading-link href=#improving-model-performance-with-cleanlab><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=can-we-improve-performance-on-baseline-models>Can we improve performance on baseline models?
<a class=heading-link href=#can-we-improve-performance-on-baseline-models><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To offer some inspiration for this, the data that I used comes from a project I did my junior year at MIT. A group of friends and I took it upon ourselves to try and &ldquo;beat&rdquo; Vegas at predicting NFL game outcomes. Using their &ldquo;line&rdquo; to determine which team they believed would win, we used our classification model (a GBC) to make our predictions. To save you time from reading the non-existent write-up, we ended up tying them on the 2018 season and beating them by 3% on the 2019 seaosn.</p><p>As I revisit my project from years ago, I have a new tool in my arsenal and a new trick up my sleeve. Using a nifty wrapper from the open source project <a href=https://docs.cleanlab.ai/v2.0.0/index.html%20>Cleanlab</a>, I can now use any sk-learn styled model, at a presumably higher accuracy. By using their wrapper, I&rsquo;m tapping into their <a href=https://github.com/cleanlab/cleanlab/discussions/56#discussioncomment-358124>black magic</a> that identifies noise in my training set&rsquo;s labels, and trains the selected model after removing labels that exceed a certain noise threshold.</p><p>Let&rsquo;s take a look at a few different classes of models and see if the added functionality improves our accuracy.</p><h2 id=imports>Imports
<a class=heading-link href=#imports><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-style:italic># %pip install lightgbm</span>
</span></span><span style=display:flex><span><span style=font-style:italic># %pip install xgboost</span>
</span></span><span style=display:flex><span><span style=font-style:italic># %pip install catboost</span>
</span></span><span style=display:flex><span><span style=font-style:italic># %pip install lightgbm</span>
</span></span><span style=display:flex><span><span style=font-style:italic># %pip install cleanlab</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>numpy</span> <span style=font-weight:700>as</span> <span style=font-weight:700>np</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>pandas</span> <span style=font-weight:700>as</span> <span style=font-weight:700>pd</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>cleanlab.classification</span> <span style=font-weight:700>import</span> CleanLearning
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.metrics</span> <span style=font-weight:700>import</span> accuracy_score
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.model_selection</span> <span style=font-weight:700>import</span> train_test_split
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.neighbors</span> <span style=font-weight:700>import</span> KNeighborsClassifier
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.neural_network</span> <span style=font-weight:700>import</span> MLPClassifier
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.ensemble</span> <span style=font-weight:700>import</span> RandomForestClassifier
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>lightgbm</span> <span style=font-weight:700>import</span> LGBMClassifier
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>xgboost</span> <span style=font-weight:700>import</span> XGBClassifier
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>catboost</span> <span style=font-weight:700>import</span> CatBoostClassifier
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=font-weight:700>sklearn.svm</span> <span style=font-weight:700>import</span> SVC
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-style:italic>#future warning was being annoying</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=font-weight:700>warnings</span>
</span></span><span style=display:flex><span>warnings.filterwarnings(<span style=font-style:italic>&#39;ignore&#39;</span>)
</span></span></code></pre></div><h2 id=data>Data
<a class=heading-link href=#data><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Our data consists of week-by-week data from 2002 until the 2019 NFL season.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-style:italic>#import our football statistics data</span>
</span></span><span style=display:flex><span>df = pd.read_csv(<span style=font-style:italic>&#34;master-boxscore-tracker.csv&#34;</span>)
</span></span><span style=display:flex><span><span style=font-style:italic>#show our data</span>
</span></span><span style=display:flex><span>df.head()
</span></span><span style=display:flex><span><span style=font-style:italic>#note that data shown below is a small subset of our data</span>
</span></span></code></pre></div><table><thead><tr><th>Season</th><th>Week</th><th>Home Team</th><th>Home Score</th><th>Away Team</th><th>Away Score</th><th>Home Win</th><th>…</th><th>Home Total Yards</th><th>Home Yards Allowed</th><th>Away Total Yards</th></tr></thead><tbody><tr><td>2002</td><td>2</td><td>CLE</td><td>20</td><td>CIN</td><td>7</td><td>TRUE</td><td>…</td><td>411</td><td>470</td><td>203</td></tr><tr><td>2002</td><td>2</td><td>IND</td><td>13</td><td>MIA</td><td>21</td><td>FALSE</td><td>…</td><td>307</td><td>343</td><td>389</td></tr><tr><td>2002</td><td>2</td><td>DAL</td><td>21</td><td>TEN</td><td>13</td><td>TRUE</td><td>…</td><td>267</td><td>210</td><td>328</td></tr><tr><td>2002</td><td>2</td><td>CAR</td><td>31</td><td>DET</td><td>7</td><td>TRUE</td><td>…</td><td>265</td><td>289</td><td>257</td></tr><tr><td>2002</td><td>2</td><td>BAL</td><td>0</td><td>TB</td><td>25</td><td>FALSE</td><td>…</td><td>289</td><td>265</td><td>333</td></tr></tbody></table><h2 id=data-pre-processing-and-traintest-split>Data Pre-processing and Train/Test Split
<a class=heading-link href=#data-pre-processing-and-traintest-split><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Before we can test our models, we need to get our data into a format they can handle. Here, we convert our label columns to 1&rsquo;s and 0&rsquo;s, as well as slice our data to only include the numerical data we want to train our models on. We also split our data into our training and testing sets.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-style:italic>#convert T/F col to 1/0</span>
</span></span><span style=display:flex><span>df[<span style=font-style:italic>&#34;Home Win&#34;</span>] =  df[<span style=font-style:italic>&#34;Home Win&#34;</span>].astype(int)
</span></span><span style=display:flex><span><span style=font-style:italic>#only want numerical data</span>
</span></span><span style=display:flex><span>X = np.array(df.loc[:, <span style=font-style:italic>&#34;Line&#34;</span>:])
</span></span><span style=display:flex><span><span style=font-style:italic>#get labels</span>
</span></span><span style=display:flex><span>y = np.array(df[<span style=font-style:italic>&#39;Home Win&#39;</span>])
</span></span><span style=display:flex><span><span style=font-style:italic>#we will use default split for now </span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10)
</span></span></code></pre></div><h2 id=model-selection>Model Selection
<a class=heading-link href=#model-selection><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>Now let&rsquo;s see how this adaptation class does when used with a variety of different models.</p><ul><li>Basic<ul><li>KNN</li><li>SVM</li><li>MLP</li></ul></li><li>Ensemble<ul><li>RandomForest</li></ul></li><li>Boosting<ul><li>LightGBM</li><li>XGBoost</li><li>CatBoost</li></ul></li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-style:italic>#models we will be using with CL wrapper</span>
</span></span><span style=display:flex><span>models = [
</span></span><span style=display:flex><span>    <span style=font-style:italic>#basic models</span>
</span></span><span style=display:flex><span>    KNeighborsClassifier(),
</span></span><span style=display:flex><span>    SVC(probability = <span style=font-weight:700>True</span>),
</span></span><span style=display:flex><span>    MLPClassifier(),
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic>#ensemble model(s)</span>
</span></span><span style=display:flex><span>    RandomForestClassifier(), 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=font-style:italic>#boosting models</span>
</span></span><span style=display:flex><span>    LGBMClassifier(), 
</span></span><span style=display:flex><span>    XGBClassifier(), 
</span></span><span style=display:flex><span>    CatBoostClassifier(silent=<span style=font-weight:700>True</span>),   
</span></span><span style=display:flex><span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_names = [type(model).__name__ <span style=font-weight:700>for</span> model <span style=font-weight:700>in</span> models]
</span></span></code></pre></div><h2 id=model-evaluation>Model Evaluation
<a class=heading-link href=#model-evaluation><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>To utilize the Cleanlab wrapper, we simply use</p><p><code>clf = SomeSklearnClassifier()</code></p><p><code>model = CleanLearning(clf=clf)</code></p><p>You can then use any of the sk-learn methods on the <code>CleanLearning</code>object.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> test_clf(model, model_name):
</span></span><span style=display:flex><span>  <span style=font-style:italic>#iniiate our models</span>
</span></span><span style=display:flex><span>  clf = model
</span></span><span style=display:flex><span>  clf_cl = CleanLearning(clf=clf)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=font-style:italic>#fit baseline model</span>
</span></span><span style=display:flex><span>  clf.fit(X_train, y_train)
</span></span><span style=display:flex><span>  pred = clf.predict(X_test)
</span></span><span style=display:flex><span>  clf_acc = accuracy_score(y_test, pred)
</span></span><span style=display:flex><span>  clf_pct = <span style=font-style:italic>&#34;</span><span style=font-weight:700;font-style:italic>{:.2%}</span><span style=font-style:italic>&#34;</span>.format(clf_acc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=font-style:italic>#fit baseline model with Cleanlab wrapper</span>
</span></span><span style=display:flex><span>  clf_cl.fit(X_train, y_train)
</span></span><span style=display:flex><span>  pred = clf_cl.predict(X_test)
</span></span><span style=display:flex><span>  clf_cl_acc = accuracy_score(y_test, pred)
</span></span><span style=display:flex><span>  clf_cl_pct = <span style=font-style:italic>&#34;</span><span style=font-weight:700;font-style:italic>{:.2%}</span><span style=font-style:italic>&#34;</span>.format(clf_cl_acc)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=font-style:italic>#get difference in model perf</span>
</span></span><span style=display:flex><span>  delta = clf_cl_acc-clf_acc
</span></span><span style=display:flex><span>  delta_pct = <span style=font-style:italic>&#34;</span><span style=font-weight:700;font-style:italic>{:.2%}</span><span style=font-style:italic>&#34;</span>.format(delta)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=font-style:italic>#print results</span>
</span></span><span style=display:flex><span>  print(<span style=font-style:italic>&#34;</span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic> accuracy: </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>&#34;</span>.format(model_name, clf_pct))
</span></span><span style=display:flex><span>  print(<span style=font-style:italic>&#34;</span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic> w/ cl accuracy: </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>&#34;</span>.format(model_name, clf_cl_pct))
</span></span><span style=display:flex><span>  print(<span style=font-style:italic>&#34;Cleanlab improvement: </span><span style=font-weight:700;font-style:italic>{}</span><span style=font-style:italic>&#34;</span>.format(delta_pct))
</span></span><span style=display:flex><span>  print(<span style=font-style:italic>&#34;---------------------------------------------&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>for</span> (model, model_name) <span style=font-weight:700>in</span> zip(models, model_names):
</span></span><span style=display:flex><span>  test_clf(model, model_name)
</span></span></code></pre></div><pre><code>KNeighborsClassifier accuracy: 56.45%
KNeighborsClassifier w/ cl accuracy: 60.63%
Cleanlab improvement: 4.19%
---------------------------------------------
SVC accuracy: 57.87%
SVC w/ cl accuracy: 63.00%
Cleanlab improvement: 5.13%
---------------------------------------------
MLPClassifier accuracy: 56.72%
MLPClassifier w/ cl accuracy: 58.95%
Cleanlab improvement: 2.23%
---------------------------------------------
RandomForestClassifier accuracy: 64.62%
RandomForestClassifier w/ cl accuracy: 66.10%
Cleanlab improvement: 1.49%
---------------------------------------------
LGBMClassifier accuracy: 64.48%
LGBMClassifier w/ cl accuracy: 65.50%
Cleanlab improvement: 1.01%
---------------------------------------------
XGBClassifier accuracy: 61.31%
XGBClassifier w/ cl accuracy: 64.42%
Cleanlab improvement: 3.11%
---------------------------------------------
CatBoostClassifier accuracy: 65.09%
CatBoostClassifier w/ cl accuracy: 65.02%
Cleanlab improvement: -0.07%
---------------------------------------------
</code></pre><h2 id=results>Results
<a class=heading-link href=#results><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><p>We see considerable increases in some of the models. These deltas change on each iteration quite significantly, so further testing will be necessary to determine the average change that the cl wrapper produces. We can say, however, with confidence that the added &ldquo;black magic&rdquo; does in fact increase performance on baseline models. By reducing noise in the input data, most our models are able to predict at higher accuracies. It&rsquo;s also important to note that all of these models are run with default hyperparameters. Further work would need to be done in order to tune each model individually and then apply the cl wrapper to determine if the same increase deltas exist.</p><h2 id=further-work>Further Work
<a class=heading-link href=#further-work><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><ul><li>Tune each model to its optimal state (grid search for hyperparams) and then check delta with CL wrapper</li><li>Include visualizations for models</li><li>Include more classification models</li><li>Try with non-tabular data such as image or text data (VGG16, ResNet50, GPT3, etc.)</li><li>Additional written content</li><li>Data pre-processing like PCA or variance analysis</li></ul></div><footer></footer></article></section></div><footer class=footer><section class=container>©
1998 -
2022
Christopher Mauck
·
Powered by <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.236049395dc3682fb2719640872958e12f1f24067bb09c327b233e6290c7edac.js integrity="sha256-I2BJOV3DaC+ycZZAhylY4S8fJAZ7sJwyeyM+YpDH7aw="></script></body></html>